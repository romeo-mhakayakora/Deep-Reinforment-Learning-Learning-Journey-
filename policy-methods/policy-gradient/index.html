<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Policy Gradient — DeepRL</title>
  <meta name="description" content="Policy gradient methods." />
  <link rel="stylesheet" href="../../assets/styles.css" />
</head>
<body>
  <a class="skip" href="#main">Skip to content</a>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Policy Gradient & Action-Space Notes</title>

  <!-- MathJax for correct scientific formula rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 20px;
      color: #111;
    }
    h1 { border-bottom: 3px solid #000; padding-bottom: 8px; }
    h2 { margin-top: 30px; border-bottom: 2px solid #222; padding-bottom: 4px; }
    h3 { margin-top: 20px; }
    code {
      background: #f4f4f4;
      padding: 3px 6px;
      border-radius: 4px;
      font-size: 0.95em;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin-top: 10px;
    }
    table, th, td {
      border: 1px solid #000;
      padding: 8px;
    }
    th { background: #eee; }
  </style>
</head>
<body>

<h1>Policy Gradient, Probability & Action-Space in Reinforcement Learning</h1>

<!-- ===================================================== -->
<h2>1. Value-Based vs Policy-Based Learning</h2>

<h3>1.1 Deep Q-Network (DQN) — Value Learning</h3>
<ul>
  <li>DQN learns <strong>action-values</strong>: \(Q(s,a)\)</li>
  <li>Action is chosen using an external strategy (e.g., <code>epsilon-greedy</code>)</li>
  <li>Training stability requires additional engineering:
      <ul>
        <li>Experience Replay</li>
        <li>Target Networks</li>
        <li>Other stabilization tricks in academic literature</li>
      </ul>
  </li>
</ul>

<h3>1.2 Policy Network — Direct Action Sampling</h3>
<ul>
  <li>Instead of learning \(Q(s,a)\), the network directly learns a policy:
      \[
        \pi_\theta : s \rightarrow P(a|s)
      \]
  </li>
  <li>Outputs a probability distribution over actions</li>
  <li>Action is selected by sampling from \(P(A|S)\)</li>
  <li>No need for <code>epsilon-greedy</code> since exploration comes from sampling</li>
  <li>Often simplifies training complexity compared to DQN</li>
</ul>

<!-- ...rest of policy-gradient.html content... -->
</body>
</html>
