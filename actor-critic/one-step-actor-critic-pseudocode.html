<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>One-Step Online Actor-Critic  Pseudocode</title>
  <meta name="description" content="One-step online actor-critic (TD(0)) pseudocode." />
  <link rel="stylesheet" href="../assets/styles.css" />
</head>
<body>
  <a class="skip" href="#main">Skip to content</a>

  <header class="site-header">
    <div class="container">
      <div class="brand">
        <p class="brand-title">DeepRL</p>
        <p class="brand-subtitle">actor-critic</p>
      </div>
      <nav class="nav" aria-label="Primary">
        <a href="../">Home</a>
        <a href="./">Actor-Critic</a>
        <a href="../policy-methods/">Policy Methods</a>
        <a href="../value-methods/">Value Methods</a>
        <a href="./one-step-actor-critic-pseudocode.html" aria-current="page">One-step AC (TD(0))</a>
      </nav>
    </div>
  </header>

  <main id="main" class="container content">
    <article class="prose">
      <h1>One-Step Online Actor-Critic (TD(0), Online)  Pseudocode</h1>
      <p class="meta">A minimal online actor-critic loop using the TD error as the advantage.</p>

      <section class="codeblock" aria-label="Pseudocode">
        <pre><code>gamma  = 0.9  # Discount factor

for i in epochs:

    state = environment.get_state()

    value  = critic(state)
    policy = actor(state)
    action = policy.sample()

    next_state, reward = environment.take_action(action)
    next_value = critic(next_state)

    advantage = reward + gamma * next_value - value  # TD error (1-step)

    # Note: In online Actor-Critic, this 1-step advantage is equivalent to TD(0) error.
    # Monte-Carlo AC uses a different return-based advantage formulation.

    loss_actor  = - policy.log_prob(action) * advantage  # Policy gradient objective
    loss_critic = advantage ** 2                         # Critic update (TD error MSE)

    update_actor(loss_actor)
    update_critic(loss_critic)

# The loop is online: updates occur immediately after each transition.
# 1-step TD advantage is valid for TD(0) AC, but not universal for GAE or n-step advantages.</code></pre>
      </section>
    </article>
  </main>

  <footer class="site-footer">
    <div class="container">
      <div class="meta">DeepRL  <a href="./">Back to actor-critic</a>  <a href="../">Home</a></div>
    </div>
  </footer>
</body>
</html>
